{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = pd.read_csv('data/emb_data4.csv')\n",
    "feat = shuffle(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = feat['master_id']\n",
    "X2 = feat.drop(['master_id', 'assigne_state'], axis=1)\n",
    "X2 = scale(X2, axis=0)\n",
    "y = feat['assigne_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_masters = []\n",
    "\n",
    "for i in range(len(X1)):\n",
    "    if X1[i] not in u_masters:\n",
    "        u_masters.append(X1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113184/113184 [02:13<00:00, 849.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113184, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "X_new = pd.DataFrame(columns=['master_idx'], index = X1.index)\n",
    "for i in tqdm(range(len(X1))):\n",
    "    X_new['master_idx'][i] = u_masters.index(X1[i])\n",
    "        \n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tst_X_new = X_new[0:20000].as_matrix()\n",
    "tst_X2 = X2[0:20000]\n",
    "tst_y = y[0:20000]\n",
    "\n",
    "val_X_new = X_new[20001:50000].as_matrix()\n",
    "val_X2 = X2[20001:50000]\n",
    "val_y = y[20001:50000]\n",
    "\n",
    "trn_X_new = X_new[50001:].as_matrix()\n",
    "trn_X2 = X2[50001:]\n",
    "trn_y = y[50001:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_X_new, val_X_new, trn_X2, val_X2, trn_y, val_y = train_test_split(X_new, X2, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, merge\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import concatenate\n",
    "n_masters = X1.nunique()\n",
    "n_factors = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_in = Input(shape=(1,), dtype='int64', name='master_in')\n",
    "m1 = Embedding(n_masters, n_factors, input_length=1)(master_in)\n",
    "m2 = Flatten()(m1)\n",
    "order_in = Input(shape=(12,), dtype='float32', name='order_in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x =merge([m2, order_in], mode = 'concat')\n",
    "x = concatenate([m2, order_in])\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(200, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(1, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "master_in (InputLayer)           (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 1, 40)         117920      master_in[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 40)            0           embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "order_in (InputLayer)            (None, 12)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)      (None, 52)            0           flatten_3[0][0]                  \n",
      "                                                                   order_in[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 52)            208         concatenate_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 52)            0           batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 200)           10600       dropout_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 200)           800         dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 200)           0           batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 100)           20100       dropout_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, 100)           400         dense_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)              (None, 100)           0           batch_normalization_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 1)             101         dropout_9[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 150,129\n",
      "Trainable params: 149,425\n",
      "Non-trainable params: 704\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([master_in, order_in], x)\n",
    "model.compile(loss='binary_crossentropy', optimizer=SGD(lr=0.1), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.optimizer.lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63183 samples, validate on 29999 samples\n",
      "Epoch 1/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.4052 - acc: 0.8161 - val_loss: 0.4339 - val_acc: 0.8054\n",
      "Epoch 2/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.4054 - acc: 0.8155 - val_loss: 0.4339 - val_acc: 0.8055\n",
      "Epoch 3/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.4042 - acc: 0.8162 - val_loss: 0.4339 - val_acc: 0.8054\n",
      "Epoch 4/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.4044 - acc: 0.8176 - val_loss: 0.4339 - val_acc: 0.8055\n",
      "Epoch 5/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.4060 - acc: 0.8163 - val_loss: 0.4339 - val_acc: 0.8055\n",
      "Epoch 6/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.4055 - acc: 0.8172 - val_loss: 0.4339 - val_acc: 0.8054\n",
      "Epoch 7/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.4042 - acc: 0.8178 - val_loss: 0.4339 - val_acc: 0.8056\n",
      "Epoch 8/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.4036 - acc: 0.8166 - val_loss: 0.4339 - val_acc: 0.8056\n",
      "Epoch 9/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.4052 - acc: 0.8154 - val_loss: 0.4339 - val_acc: 0.8057\n",
      "Epoch 10/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.4046 - acc: 0.8164 - val_loss: 0.4339 - val_acc: 0.8056\n",
      "Epoch 11/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.4056 - acc: 0.8156 - val_loss: 0.4339 - val_acc: 0.8054\n",
      "Epoch 12/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.4049 - acc: 0.8168 - val_loss: 0.4339 - val_acc: 0.8057\n",
      "Epoch 13/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.4034 - acc: 0.8173 - val_loss: 0.4339 - val_acc: 0.8057\n",
      "Epoch 14/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.4053 - acc: 0.8161 - val_loss: 0.4339 - val_acc: 0.8056\n",
      "Epoch 15/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.4039 - acc: 0.8173 - val_loss: 0.4339 - val_acc: 0.8056\n",
      "Epoch 16/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.4068 - acc: 0.8153 - val_loss: 0.4339 - val_acc: 0.8057\n",
      "Epoch 17/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.4039 - acc: 0.8177 - val_loss: 0.4339 - val_acc: 0.8056\n",
      "Epoch 18/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.4040 - acc: 0.8179 - val_loss: 0.4339 - val_acc: 0.8055\n",
      "Epoch 19/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.4052 - acc: 0.8162 - val_loss: 0.4339 - val_acc: 0.8056\n",
      "Epoch 20/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.4041 - acc: 0.8188 - val_loss: 0.4339 - val_acc: 0.8056\n",
      "Epoch 21/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.4050 - acc: 0.8167 - val_loss: 0.4339 - val_acc: 0.8056\n",
      "Epoch 22/1000\n",
      "61440/63183 [============================>.] - ETA: 0s - loss: 0.4034 - acc: 0.8158\n",
      "Epoch 00021: reducing learning rate to 0.0008100000210106373.\n",
      "63183/63183 [==============================] - 4s - loss: 0.4035 - acc: 0.8158 - val_loss: 0.4339 - val_acc: 0.8058\n",
      "Epoch 23/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.4055 - acc: 0.8168 - val_loss: 0.4339 - val_acc: 0.8058\n",
      "Epoch 24/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.4041 - acc: 0.8163 - val_loss: 0.4339 - val_acc: 0.8058\n",
      "Epoch 25/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.4045 - acc: 0.8176 - val_loss: 0.4339 - val_acc: 0.8057\n",
      "Epoch 26/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.4050 - acc: 0.8168 - val_loss: 0.4339 - val_acc: 0.8057\n",
      "Epoch 27/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.4041 - acc: 0.8164 - val_loss: 0.4339 - val_acc: 0.8057\n",
      "Epoch 28/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.4025 - acc: 0.8178 - val_loss: 0.4339 - val_acc: 0.8056\n",
      "Epoch 29/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.4049 - acc: 0.8164 - val_loss: 0.4339 - val_acc: 0.8057\n",
      "Epoch 30/1000\n",
      "61440/63183 [============================>.] - ETA: 0s - loss: 0.4039 - acc: 0.8159\n",
      "Epoch 00029: reducing learning rate to 0.00024299999931827186.\n",
      "63183/63183 [==============================] - 3s - loss: 0.4041 - acc: 0.8158 - val_loss: 0.4339 - val_acc: 0.8056\n",
      "Epoch 31/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.4043 - acc: 0.8183 - val_loss: 0.4339 - val_acc: 0.8056\n",
      "Epoch 32/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.4061 - acc: 0.8167 - val_loss: 0.4339 - val_acc: 0.8056\n",
      "Epoch 33/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.4038 - acc: 0.8164 - val_loss: 0.4339 - val_acc: 0.8056\n",
      "Epoch 34/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.4050 - acc: 0.8176 - val_loss: 0.4339 - val_acc: 0.8055\n",
      "Epoch 35/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.4042 - acc: 0.8170 - val_loss: 0.4339 - val_acc: 0.8056\n",
      "Epoch 36/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.4056 - acc: 0.8168 - val_loss: 0.4339 - val_acc: 0.8056\n",
      "Epoch 37/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.4062 - acc: 0.8161 - val_loss: 0.4339 - val_acc: 0.8056\n",
      "Epoch 38/1000\n",
      "61440/63183 [============================>.] - ETA: 0s - loss: 0.4041 - acc: 0.8170\n",
      "Epoch 00037: reducing learning rate to 7.290000066859647e-05.\n",
      "63183/63183 [==============================] - 4s - loss: 0.4042 - acc: 0.8168 - val_loss: 0.4339 - val_acc: 0.8055\n",
      "Epoch 39/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.4041 - acc: 0.8182 - val_loss: 0.4339 - val_acc: 0.8056\n",
      "Epoch 40/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.4054 - acc: 0.8169 - val_loss: 0.4339 - val_acc: 0.8056\n",
      "Epoch 41/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.4049 - acc: 0.8159 - val_loss: 0.4339 - val_acc: 0.8057\n",
      "Epoch 42/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.4049 - acc: 0.8162 - val_loss: 0.4339 - val_acc: 0.8055\n",
      "Epoch 43/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.4046 - acc: 0.8172 - val_loss: 0.4339 - val_acc: 0.8057\n",
      "Epoch 44/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.4042 - acc: 0.8180 - val_loss: 0.4339 - val_acc: 0.8057\n",
      "Epoch 00043: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x113291b70>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([trn_X_new, trn_X2], trn_y, batch_size = 1024, epochs=1000,\n",
    "          validation_data=([val_X_new, val_X2], val_y),\n",
    "          callbacks = [EarlyStopping(monitor='val_acc', patience=30, verbose=1, min_delta=1e-4, mode='max'),\n",
    "                           #ReduceLROnPlateau(monitor='val_acc', factor=0.3, patience=5, verbose=1, epsilon=1e-4, mode='max')\n",
    "                            CSVLogger('data/learning_log.csv', separator=',', append=False),\n",
    "                            ReduceLROnPlateau(monitor='val_acc', factor=0.3, patience=8, min_lr=0.00001, verbose = 1)\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict([tst_X_new, tst_X2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76589642311042894"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(tst_y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = y_pred.reshape((20000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "group_names = ['0-10', '10-20', '20-30', '30-40', '40-50', '50-60', '60-70', '70-80', '80-90', '90-100']\n",
    "\n",
    "categories = pd.cut(y_pred2, bins, labels=group_names)\n",
    "\n",
    "df = pd.DataFrame(columns=['y_true', 'y_pred', 'bin'])\n",
    "df['y_true'] = tst_y\n",
    "df['y_pred'] = y_pred2\n",
    "df['bin'] = categories\n",
    "\n",
    "df.to_csv('bins.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_masters = X.nunique()\n",
    "n_factors = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = Sequential([\n",
    "    Embedding(n_masters, n_factors, input_length=1),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model.compile(loss='binary_crossentropy', optimizer=SGD(lr=0.001), metrics=['accuracy'])\n",
    "emb_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model.fit(X_new['master_idx'], y, batch_size=256, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
