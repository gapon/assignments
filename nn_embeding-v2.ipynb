{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/emb_data4.csv')\n",
    "data = shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_raw = data['master_id']\n",
    "X2_raw = data.drop(['master_id', 'assigne_state'], axis=1)\n",
    "\n",
    "# Need to save\n",
    "X2_mean = X2_raw.mean(axis = 0)\n",
    "X2_std = X2_raw.std(axis = 0)\n",
    "\n",
    "# Scaling\n",
    "X2 = (X2_raw - X2_mean)/X2_std\n",
    "X2 = X2.fillna(0)\n",
    "\n",
    "y = data['assigne_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113184/113184 [00:04<00:00, 26572.74it/s]\n"
     ]
    }
   ],
   "source": [
    "X1_lookup = [-1]\n",
    "for i in tqdm(range(len(X1_raw))):\n",
    "    if X1_raw[i] not in X1_lookup:\n",
    "        X1_lookup.append(X1_raw[i])\n",
    "# X1_lookup need to save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113184/113184 [02:34<00:00, 733.89it/s]\n"
     ]
    }
   ],
   "source": [
    "X1 = pd.DataFrame(columns=['master_idx'], index = X1_raw.index)\n",
    "\n",
    "for i in tqdm(range(len(X1_raw))):\n",
    "    if X1_raw[i] in X1_lookup:\n",
    "        X1['master_idx'][i] = X1_lookup.index(X1_raw[i])\n",
    "    else:\n",
    "        X1['master_idx'][i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_tst = X1[0:20000].as_matrix()\n",
    "X2_tst = X2[0:20000].as_matrix()\n",
    "y_tst = y[0:20000]#.as_matrix().reshape(-1,1)\n",
    "\n",
    "X1_val = X1[20001:50000].as_matrix()\n",
    "X2_val = X2[20001:50000].as_matrix()\n",
    "y_val = y[20001:50000]#.as_matrix().reshape(-1,1)\n",
    "\n",
    "X1_trn = X1[50001:].as_matrix()\n",
    "X2_trn = X2[50001:].as_matrix()\n",
    "y_trn = y[50001:]#.as_matrix().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X1_trn, X1_val, X2_trn, X2_val, y_trn, y_val = train_test_split(X1, X2, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, merge\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import concatenate\n",
    "from sklearn.metrics import roc_auc_score\n",
    "n_masters = len(X1_lookup)\n",
    "n_factors = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_in = Input(shape=(1,), dtype='int64', name='master_in')\n",
    "m1 = Embedding(n_masters, n_factors, input_length=1)(master_in)\n",
    "m2 = Flatten()(m1)\n",
    "order_in = Input(shape=(12,), dtype='float32', name='order_in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = concatenate([m2, order_in])\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(200, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(1, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "master_in (InputLayer)           (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 1, 50)         147450      master_in[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 50)            0           embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "order_in (InputLayer)            (None, 12)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 62)            0           flatten_1[0][0]                  \n",
      "                                                                   order_in[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 62)            248         concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 62)            0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 200)           12600       dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 200)           800         dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 200)           0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 100)           20100       dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 100)           400         dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 100)           0           batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1)             101         dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 181,699\n",
      "Trainable params: 180,975\n",
      "Non-trainable params: 724\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([master_in, order_in], x)\n",
    "model.compile(loss='binary_crossentropy', optimizer=SGD(lr=0.1), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63183 samples, validate on 29999 samples\n",
      "Epoch 1/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.3120 - acc: 0.5297 - val_loss: 0.7072 - val_acc: 0.4253\n",
      "Epoch 2/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.2791 - acc: 0.5510 - val_loss: 0.7189 - val_acc: 0.3824\n",
      "Epoch 3/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.2723 - acc: 0.5652 - val_loss: 0.7241 - val_acc: 0.3998\n",
      "Epoch 4/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.2685 - acc: 0.5801 - val_loss: 0.7078 - val_acc: 0.4750\n",
      "Epoch 5/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.2642 - acc: 0.5883 - val_loss: 0.7136 - val_acc: 0.4711\n",
      "Epoch 6/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.2610 - acc: 0.5988 - val_loss: 0.7048 - val_acc: 0.4877\n",
      "Epoch 7/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.2572 - acc: 0.6104 - val_loss: 0.6867 - val_acc: 0.5287\n",
      "Epoch 8/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.2532 - acc: 0.6218 - val_loss: 0.6700 - val_acc: 0.5570\n",
      "Epoch 9/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.2498 - acc: 0.6323 - val_loss: 0.6600 - val_acc: 0.5727\n",
      "Epoch 10/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.2473 - acc: 0.6452 - val_loss: 0.6650 - val_acc: 0.5671\n",
      "Epoch 11/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.2444 - acc: 0.6530 - val_loss: 0.6527 - val_acc: 0.5911\n",
      "Epoch 12/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.2428 - acc: 0.6580 - val_loss: 0.6367 - val_acc: 0.6129\n",
      "Epoch 13/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.2414 - acc: 0.6638 - val_loss: 0.6353 - val_acc: 0.6143\n",
      "Epoch 14/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2398 - acc: 0.6642 - val_loss: 0.6476 - val_acc: 0.6008\n",
      "Epoch 15/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2388 - acc: 0.6698 - val_loss: 0.6509 - val_acc: 0.5931\n",
      "Epoch 16/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2379 - acc: 0.6722 - val_loss: 0.6291 - val_acc: 0.6221\n",
      "Epoch 17/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2361 - acc: 0.6736 - val_loss: 0.6267 - val_acc: 0.6274\n",
      "Epoch 18/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2348 - acc: 0.6810 - val_loss: 0.6429 - val_acc: 0.6098\n",
      "Epoch 19/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2347 - acc: 0.6788 - val_loss: 0.6263 - val_acc: 0.6264\n",
      "Epoch 20/1000\n",
      "63183/63183 [==============================] - 3s - loss: 0.2340 - acc: 0.6798 - val_loss: 0.6271 - val_acc: 0.6281\n",
      "Epoch 21/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2316 - acc: 0.6836 - val_loss: 0.6335 - val_acc: 0.6195\n",
      "Epoch 22/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2323 - acc: 0.6855 - val_loss: 0.6325 - val_acc: 0.6224\n",
      "Epoch 23/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2318 - acc: 0.6841 - val_loss: 0.6178 - val_acc: 0.6352\n",
      "Epoch 24/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2305 - acc: 0.6864 - val_loss: 0.6063 - val_acc: 0.6503\n",
      "Epoch 25/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2297 - acc: 0.6890 - val_loss: 0.6226 - val_acc: 0.6336\n",
      "Epoch 26/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2306 - acc: 0.6899 - val_loss: 0.6198 - val_acc: 0.6365\n",
      "Epoch 27/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2290 - acc: 0.6877 - val_loss: 0.6277 - val_acc: 0.6252\n",
      "Epoch 28/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2291 - acc: 0.6898 - val_loss: 0.6227 - val_acc: 0.6333\n",
      "Epoch 29/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2289 - acc: 0.6896 - val_loss: 0.6218 - val_acc: 0.6317\n",
      "Epoch 30/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2285 - acc: 0.6890 - val_loss: 0.6131 - val_acc: 0.6436\n",
      "Epoch 31/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2273 - acc: 0.6908 - val_loss: 0.6211 - val_acc: 0.6357\n",
      "Epoch 32/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2270 - acc: 0.6915 - val_loss: 0.6165 - val_acc: 0.6389\n",
      "Epoch 33/1000\n",
      "62976/63183 [============================>.] - ETA: 0s - loss: 0.2273 - acc: 0.6907\n",
      "Epoch 00032: reducing learning rate to 0.030000000447034835.\n",
      "63183/63183 [==============================] - 4s - loss: 0.2272 - acc: 0.6907 - val_loss: 0.6138 - val_acc: 0.6419\n",
      "Epoch 34/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2261 - acc: 0.6947 - val_loss: 0.6189 - val_acc: 0.6365\n",
      "Epoch 35/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2258 - acc: 0.6953 - val_loss: 0.6174 - val_acc: 0.6389\n",
      "Epoch 36/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2254 - acc: 0.6967 - val_loss: 0.6158 - val_acc: 0.6415\n",
      "Epoch 37/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2265 - acc: 0.6964 - val_loss: 0.6143 - val_acc: 0.6429\n",
      "Epoch 38/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2257 - acc: 0.6965 - val_loss: 0.6137 - val_acc: 0.6420\n",
      "Epoch 39/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2258 - acc: 0.6958 - val_loss: 0.6131 - val_acc: 0.6424\n",
      "Epoch 40/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2251 - acc: 0.6963 - val_loss: 0.6146 - val_acc: 0.6408\n",
      "Epoch 41/1000\n",
      "62464/63183 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.6937- ETA: 0s - loss: 0.2258 - acc: 0.69\n",
      "Epoch 00040: reducing learning rate to 0.009000000357627868.\n",
      "63183/63183 [==============================] - 4s - loss: 0.2258 - acc: 0.6937 - val_loss: 0.6139 - val_acc: 0.6417\n",
      "Epoch 42/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2253 - acc: 0.6946 - val_loss: 0.6157 - val_acc: 0.6406\n",
      "Epoch 43/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2246 - acc: 0.6962 - val_loss: 0.6158 - val_acc: 0.6409\n",
      "Epoch 44/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2247 - acc: 0.6976 - val_loss: 0.6156 - val_acc: 0.6408\n",
      "Epoch 45/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2244 - acc: 0.6981 - val_loss: 0.6151 - val_acc: 0.6418\n",
      "Epoch 46/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2242 - acc: 0.6964 - val_loss: 0.6169 - val_acc: 0.6397\n",
      "Epoch 47/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2243 - acc: 0.6956 - val_loss: 0.6185 - val_acc: 0.6384\n",
      "Epoch 48/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2252 - acc: 0.6964 - val_loss: 0.6167 - val_acc: 0.6401\n",
      "Epoch 49/1000\n",
      "62464/63183 [============================>.] - ETA: 0s - loss: 0.2244 - acc: 0.6963\n",
      "Epoch 00048: reducing learning rate to 0.002700000163167715.\n",
      "63183/63183 [==============================] - 4s - loss: 0.2246 - acc: 0.6961 - val_loss: 0.6155 - val_acc: 0.6415\n",
      "Epoch 50/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2245 - acc: 0.6973 - val_loss: 0.6152 - val_acc: 0.6416\n",
      "Epoch 51/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2243 - acc: 0.6973 - val_loss: 0.6158 - val_acc: 0.6408\n",
      "Epoch 52/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2237 - acc: 0.6983 - val_loss: 0.6155 - val_acc: 0.6411\n",
      "Epoch 53/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2245 - acc: 0.6975 - val_loss: 0.6163 - val_acc: 0.6405\n",
      "Epoch 54/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2242 - acc: 0.6980 - val_loss: 0.6162 - val_acc: 0.6407\n",
      "Epoch 55/1000\n",
      "63183/63183 [==============================] - 4s - loss: 0.2241 - acc: 0.6990 - val_loss: 0.6169 - val_acc: 0.6403\n",
      "Epoch 00054: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12327df60>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight = {1: 1.0, 0: y[y == 1].size / y[y == 0].size}\n",
    "\n",
    "model.fit([X1_trn, X2_trn], y_trn, batch_size = 512, epochs=1000,\n",
    "          validation_data=([X1_val, X2_val], y_val),\n",
    "          class_weight = class_weight,\n",
    "          callbacks = [EarlyStopping(monitor='val_acc', patience=30, verbose=1, min_delta=1e-4, mode='max'),\n",
    "                       #ReduceLROnPlateau(monitor='val_acc', factor=0.3, patience=5, verbose=1, epsilon=1e-4, mode='max')\n",
    "                        CSVLogger('data/learning_log.csv', separator=',', append=False),\n",
    "                        ReduceLROnPlateau(monitor='val_acc', factor=0.3, patience=8, min_lr=0.00001, verbose = 1)\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.753977008509\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict([X1_tst, X2_tst])\n",
    "print(roc_auc_score(y_tst, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('data/model_01.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump preprocessing to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import cPickle as pickle\n",
    "\n",
    "pickle_file = 'data/preprocessing.pickle'\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'X1_lookup': X1_lookup,\n",
    "        'X2_mean': X2_mean,\n",
    "        'X2_std': X2_std\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred2 = y_pred.reshape((20000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "group_names = ['0-10', '10-20', '20-30', '30-40', '40-50', '50-60', '60-70', '70-80', '80-90', '90-100']\n",
    "\n",
    "categories = pd.cut(y_pred2, bins, labels=group_names)\n",
    "\n",
    "df = pd.DataFrame(columns=['y_true', 'y_pred', 'bin'])\n",
    "df['y_true'] = tst_y\n",
    "df['y_pred'] = y_pred2\n",
    "df['bin'] = categories\n",
    "\n",
    "df.to_csv('bins.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_masters = X.nunique()\n",
    "n_factors = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_model = Sequential([\n",
    "    Embedding(n_masters, n_factors, input_length=1),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_model.compile(loss='binary_crossentropy', optimizer=SGD(lr=0.001), metrics=['accuracy'])\n",
    "emb_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_model.fit(X_new['master_idx'], y, batch_size=256, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
